{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of GloVe Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashraf-ul/DeepLearningWithTensorflow/blob/master/Copy_of_GloVe_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBHZ2O-MFK1",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis with GloVe Vectors\n",
        "\n",
        "Sentiment analysis is the process of categorizing opinions in natural language text. Several methods can be used to estimate sentiment. In this example, labeled reviews from Yelp, Amazon, and IMDB are used to train a supervised binary classification model. The model is a 1D convolutional neural network (CNN). While 2D CNNs are commonly used for image classification, their exceptional spatial capabilities can applied to text in one dimension.\n",
        "\n",
        "Words in a sentence must be encoded as vectors for training and prediction. This encoding is more commonly called embedding. A straightforward approach would assign every distinct word a unique numerical value. A better approach is use pretrained word embeddings based on a large corpus of text, such as Wikipedia. Global Vectors for Word Representation ([GloVe](https://nlp.stanford.edu/projects/glove/)) is a popular vector representation based on word co-occurance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2MMR98QMFK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVAIpfO0MFK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_eQNboMFLA",
        "colab_type": "text"
      },
      "source": [
        "# Training Data\n",
        "\n",
        "Data is loaded into a Pandas data frame from text files. This dataset contains two columns: a natural language comment and binary positive/negative sentiment represented as 1 or 0.\n",
        "\n",
        "The data is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyrqXX4M5ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "!wget --quiet \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "!unzip -q \"sentiment labelled sentences\"\n",
        "!mv \"sentiment labelled sentences\" data\n",
        "!ls -l data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2pC__fWj2C0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat data/readme.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je5Lkv_aMFLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = ['data/yelp_labelled.txt', 'data/amazon_cells_labelled.txt', 'data/imdb_labelled.txt']\n",
        "\n",
        "df_list = []\n",
        "for file in files:\n",
        "    df = pd.read_csv(file, names=['comment', 'sentiment'], sep='\\t')\n",
        "    df_list.append(df)\n",
        "\n",
        "trainingData = pd.concat(df_list)\n",
        "\n",
        "print('Number of rows: %d' % len(trainingData))\n",
        "\n",
        "trainingData.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq6cMhkEMFLG",
        "colab_type": "text"
      },
      "source": [
        "# GloVE Embeddings\n",
        "\n",
        "These are some convenience functions for loading GloVE vectors and creating an embedding matrix.\n",
        "\n",
        "The GloVe vector files are downloaded from [Stanford](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGcwMGR-R3tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download GloVe embeddings\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!echo \"Unzipping glove.6B.zip\"\n",
        "!unzip -q glove.6B.zip\n",
        "!echo \"All done!\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVeUb4AsMFLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "def load_glove_vectors():\n",
        "    print('Loading glove vectors...')\n",
        "    glove_map = {}\n",
        "    with open('glove.6B.%dd.txt' % EMBEDDING_DIM, encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            glove_map[word] = np.asarray(values[1:], dtype='float32')\n",
        "    return glove_map\n",
        "\n",
        "def create_embedding_matrix(word_index, num_words):\n",
        "    glove_map = load_glove_vectors()\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        vector = glove_map.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[i] = vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxQxZ2NMFLI",
        "colab_type": "text"
      },
      "source": [
        "# Encode Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7DV0_1XMFLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = trainingData.comment.astype(str).tolist()\n",
        "sentiments = trainingData.sentiment.tolist()\n",
        "labels = np.asarray(sentiments)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(comments)\n",
        "sequences = tokenizer.texts_to_sequences(comments)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "num_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZlja3d1MFLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = create_embedding_matrix(word_index, num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSCKoWPYMFLN",
        "colab_type": "text"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "This is a 1D CNN model with a Keras embedding layer using the embedding matrix created above. The embedding layer is pre-trained, so it will not be trained here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0IUw1VDMFLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Conv1D(128, 5, activation='relu', padding='same', strides=2))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUMaXNjMMFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
        "\n",
        "# Train model for a given number of epochs\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=40, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate model against test data\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVwi0vLMFLS",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i423WNQMFLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(padded_sequences)\n",
        "most_likely = predictions.argmax(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ve2a-roMFLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = random.randrange(len(predictions))\n",
        "print(comments[index])\n",
        "print('Prediction: %d, label: %d' % (most_likely[index], sentiments[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcR3Qu6FMFLX",
        "colab_type": "text"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aMVD5lMFLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10000):\n",
        "    index = random.randrange(len(predictions))\n",
        "    if most_likely[index] != sentiments[index]:\n",
        "        break\n",
        "\n",
        "print(comments[index])\n",
        "print('Prediction: %d, label: %d' % (most_likely[index], sentiments[index]))\n",
        "\n",
        "plt.bar(range(num_classes), predictions[index], tick_label=range(num_classes))\n",
        "plt.title('Prediction values')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMplVIcbMFLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}