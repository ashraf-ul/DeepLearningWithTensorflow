{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of GloVe Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashraf-ul/DeepLearningWithTensorflow/blob/master/Copy_of_GloVe_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBHZ2O-MFK1",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis with GloVe Vectors\n",
        "\n",
        "Sentiment analysis is the process of categorizing opinions in natural language text. Several methods can be used to estimate sentiment. In this example, labeled reviews from Yelp, Amazon, and IMDB are used to train a supervised binary classification model. The model is a 1D convolutional neural network (CNN). While 2D CNNs are commonly used for image classification, their exceptional spatial capabilities can applied to text in one dimension.\n",
        "\n",
        "Words in a sentence must be encoded as vectors for training and prediction. This encoding is more commonly called embedding. A straightforward approach would assign every distinct word a unique numerical value. A better approach is use pretrained word embeddings based on a large corpus of text, such as Wikipedia. Global Vectors for Word Representation ([GloVe](https://nlp.stanford.edu/projects/glove/)) is a popular vector representation based on word co-occurance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2MMR98QMFK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVAIpfO0MFK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_eQNboMFLA",
        "colab_type": "text"
      },
      "source": [
        "# Training Data\n",
        "\n",
        "Data is loaded into a Pandas data frame from text files. This dataset contains two columns: a natural language comment and binary positive/negative sentiment represented as 1 or 0.\n",
        "\n",
        "The data is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyrqXX4M5ek",
        "colab_type": "code",
        "outputId": "d03c8ec1-2a96-43a3-a023-a2dc579e1a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Download data from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "!wget --quiet \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "!unzip -q \"sentiment labelled sentences\"\n",
        "!mv \"sentiment labelled sentences\" data\n",
        "!ls -l data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace __MACOSX/sentiment labelled sentences/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: mv: cannot move 'sentiment labelled sentences' to 'data/sentiment labelled sentences': Directory not empty\n",
            "total 212\n",
            "-rw-r--r-- 1 root root 58226 Jul  5  2016  amazon_cells_labelled.txt\n",
            "-rw-r--r-- 1 root root 85285 Feb 15  2015  imdb_labelled.txt\n",
            "-rw-r--r-- 1 root root  1070 May 31  2015  readme.txt\n",
            "drwxr-xr-x 2 root root  4096 Apr 28 12:14 'sentiment labelled sentences'\n",
            "-rw-r--r-- 1 root root 61320 Jul  5  2016  yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2pC__fWj2C0",
        "colab_type": "code",
        "outputId": "4fe0b446-20fa-4bd7-ebf8-68a2745f8dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "!cat data/readme.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n",
            "Please cite the paper if you want to use it :)\n",
            "\n",
            "It contains sentences labelled with positive or negative sentiment, extracted from reviews of products, movies, and restaurants\n",
            "\n",
            "=======\n",
            "Format:\n",
            "=======\n",
            "sentence \\t score \\n\n",
            "\n",
            "\n",
            "=======\n",
            "Details:\n",
            "=======\n",
            "Score is either 1 (for positive) or 0 (for negative)\t\n",
            "The sentences come from three different websites/fields:\n",
            "\n",
            "imdb.com\n",
            "amazon.com\n",
            "yelp.com\n",
            "\n",
            "For each website, there exist 500 positive and 500 negative sentences. Those were selected randomly for larger datasets of reviews. \n",
            "We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.\n",
            "\n",
            "\n",
            "\n",
            "For the full datasets look:\n",
            "\n",
            "imdb: Maas et. al., 2011 'Learning word vectors for sentiment analysis'\n",
            "amazon: McAuley et. al., 2013 'Hidden factors and hidden topics: Understanding rating dimensions with review text'\n",
            "yelp: Yelp dataset challenge http://www.yelp.com/dataset_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je5Lkv_aMFLA",
        "colab_type": "code",
        "outputId": "70865d55-f72e-40da-e27e-23994507ebef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "files = ['data/yelp_labelled.txt', 'data/amazon_cells_labelled.txt', 'data/imdb_labelled.txt']\n",
        "\n",
        "df_list = []\n",
        "for file in files:\n",
        "    df = pd.read_csv(file, names=['comment', 'sentiment'], sep='\\t')\n",
        "    df_list.append(df)\n",
        "\n",
        "trainingData = pd.concat(df_list)\n",
        "\n",
        "print('Number of rows: %d' % len(trainingData))\n",
        "\n",
        "trainingData.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows: 2748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Now I am getting angry and I want my damn pho.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The potatoes were like rubber and you could te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The fries were great too.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A great touch.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1\n",
              "5     Now I am getting angry and I want my damn pho.          0\n",
              "6              Honeslty it didn't taste THAT fresh.)          0\n",
              "7  The potatoes were like rubber and you could te...          0\n",
              "8                          The fries were great too.          1\n",
              "9                                     A great touch.          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq6cMhkEMFLG",
        "colab_type": "text"
      },
      "source": [
        "# GloVE Embeddings\n",
        "\n",
        "These are some convenience functions for loading GloVE vectors and creating an embedding matrix.\n",
        "\n",
        "The GloVe vector files are downloaded from [Stanford](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGcwMGR-R3tR",
        "colab_type": "code",
        "outputId": "6492417f-2849-4c3c-be54-0334729339ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "# Download GloVe embeddings\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!echo \"Unzipping glove.6B.zip\"\n",
        "!unzip -q glove.6B.zip\n",
        "!echo \"All done!\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 12:22:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-28 12:22:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-28 12:22:20--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         12%[=>                  ] 100.75M  2.23MB/s    eta 3m 19s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVeUb4AsMFLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "def load_glove_vectors():\n",
        "    print('Loading glove vectors...')\n",
        "    glove_map = {}\n",
        "    with open('glove.6B.%dd.txt' % EMBEDDING_DIM, encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            glove_map[word] = np.asarray(values[1:], dtype='float32')\n",
        "    return glove_map\n",
        "\n",
        "def create_embedding_matrix(word_index, num_words):\n",
        "    glove_map = load_glove_vectors()\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        vector = glove_map.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[i] = vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxQxZ2NMFLI",
        "colab_type": "text"
      },
      "source": [
        "# Encode Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7DV0_1XMFLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = trainingData.comment.astype(str).tolist()\n",
        "sentiments = trainingData.sentiment.tolist()\n",
        "labels = np.asarray(sentiments)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(comments)\n",
        "sequences = tokenizer.texts_to_sequences(comments)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "num_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZlja3d1MFLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = create_embedding_matrix(word_index, num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSCKoWPYMFLN",
        "colab_type": "text"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "This is a 1D CNN model with a Keras embedding layer using the embedding matrix created above. The embedding layer is pre-trained, so it will not be trained here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0IUw1VDMFLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Conv1D(128, 5, activation='relu', padding='same', strides=2))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUMaXNjMMFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
        "\n",
        "# Train model for a given number of epochs\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=40, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate model against test data\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVwi0vLMFLS",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i423WNQMFLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(padded_sequences)\n",
        "most_likely = predictions.argmax(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ve2a-roMFLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = random.randrange(len(predictions))\n",
        "print(comments[index])\n",
        "print('Prediction: %d, label: %d' % (most_likely[index], sentiments[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcR3Qu6FMFLX",
        "colab_type": "text"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aMVD5lMFLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10000):\n",
        "    index = random.randrange(len(predictions))\n",
        "    if most_likely[index] != sentiments[index]:\n",
        "        break\n",
        "\n",
        "print(comments[index])\n",
        "print('Prediction: %d, label: %d' % (most_likely[index], sentiments[index]))\n",
        "\n",
        "plt.bar(range(num_classes), predictions[index], tick_label=range(num_classes))\n",
        "plt.title('Prediction values')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMplVIcbMFLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}